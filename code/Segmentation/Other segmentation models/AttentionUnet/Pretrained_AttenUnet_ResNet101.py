# -*- coding: utf-8 -*-
"""WCEBleedGen_Challenge_pretrained_AttenUnet_ResNet101(with_val) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NFZqZawy5UCgUyJDra0z_EPWBIkjkOyW
"""

# Importing the required libraries
import os
import numpy as np
import tensorflow as tf
from keras import backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt

#installing dependencies
!pip install keras_unet_collection

#Defining validation metrics
def dice_coef(y_true, y_pred, smooth = 1):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1. - dice_coef(y_true, y_pred)

def specificity(y_true, y_pred):
    neg_y_true = 1 - y_true
    neg_y_pred = 1 - y_pred
    fp = K.sum(neg_y_true * y_pred)
    tn = K.sum(neg_y_true * neg_y_pred)
    specificity = tn / (tn + fp + K.epsilon())
    return specificity


def f1score(y_true, y_pred):

    y_true = tf.keras.backend.flatten(y_true)
    y_pred = tf.keras.backend.flatten(y_pred)
    y_true = tf.keras.backend.cast(y_true, 'float32')
    y_pred = tf.keras.backend.cast(tf.keras.backend.round(y_pred), 'float32')
    tp = tf.keras.backend.sum(y_true * y_pred)
    fp = tf.keras.backend.sum(1 - y_true * y_pred)
    fn = tf.keras.backend.sum(y_true * (1 - y_pred))
    precision = tp / (tp + fp + tf.keras.backend.epsilon())
    recall = tp / (tp + fn + tf.keras.backend.epsilon())
    return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())

def iou_coef(y_true, y_pred):
    y_pred_resized = tf.image.resize(y_pred, (224, 224))
    intersection = K.sum(K.abs(y_true * y_pred_resized), axis=[1,2,3])
    union = K.sum(y_true,[1,2,3])+K.sum(y_pred_resized,[1,2,3])-intersection
    iou = K.mean((intersection + K.epsilon()) / (union + K.epsilon()), axis=0)
    return iou

def iou_coef_loss(y_true, y_pred):
    return 1 - iou_coef(y_true, y_pred)

# Define the image and mask paths for training and validation datasets
path = "Challenge/Challenge/Train/bleed"
image_path = os.path.join(path, "images_bleed")
mask_path = os.path.join(path, "masks_bleed")

path = "Challenge/Challenge/Test/bleed"
test_image_path = os.path.join(path, "images_bleed")
test_mask_path = os.path.join(path, "masks_bleed")

# Define target size for resizing
target_size = (224, 224)

from google.colab import drive
drive.mount('/content/drive')

#Loading the dataset
def load_data(image_folder, mask_folder, target_size):
    images = []
    masks = []
    for file in os.listdir(image_folder):
        image_path = os.path.join(image_folder, file)
        mask_file = file.replace('img','ann')
        mask_path = os.path.join(mask_folder, mask_file)
        image = img_to_array(load_img(image_path, target_size=target_size))
        mask = img_to_array(load_img(mask_path, target_size=target_size))
        images.append(image)
        masks.append(mask)
    return np.array(images), np.array(masks)

train_images, train_masks = load_data(image_path, mask_path, target_size=target_size)
test_images, test_masks = load_data(test_image_path, test_mask_path, target_size=target_size)

# Converting to th single channel and reshaping
train_labels = train_masks[..., 0]  # Select the first channel
test_labels = test_masks[..., 0]  # Select the first channel
train_labels = np.expand_dims(train_labels, axis=-1)
test_labels = np.expand_dims(test_labels, axis=-1)

#Defining the model input shapes and loading the model
model = models.att_unet_2d((224, 224, 3), filter_num=[64, 128, 256, 512, 1024],
                           n_labels=1,
                           stack_num_down=2, stack_num_up=2,
                           activation='ReLU',
                           atten_activation='ReLU', attention='add',
                           output_activation='Sigmoid',
                           batch_norm=True, pool=False, unpool=False,
                           backbone='ResNet101', weights='imagenet',
                           freeze_backbone=True, freeze_batch_norm=True,
                           name='attunet')

print(model.summary())

#Defining the rate at which learning rate is reduced by a factor of 0.85
reduce_lr = ReduceLROnPlateau(monitor = "loss", factor = 0.85, patience = 1, verbose = 1, min_lr=1e-5)

#Defining model checkpoints
model_checkpoint = ModelCheckpoint('AttnUnetResNet101.h5', monitor='val_dice_loss',verbose=1, save_best_only=True)

# Compiling the model
learningRate = 1e-3
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learningRate), loss=dice_loss, metrics = [dice_coef,iou_coef, 'binary_accuracy', tf.keras.metrics.AUC(num_thresholds=100), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), specificity,dice_loss,f1score])

# Train the model(Done for 50)
history = model.fit(
    train_images, train_labels,
    validation_data=(test_images, test_labels),
    batch_size=8, epochs=50,callbacks=[reduce_lr, model_checkpoint])

#Saving the model
tf.keras.saving.save_model(model, r'AttenUnet_Resnet101.h5', overwrite=True, save_format=None)

